{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2. Classifier-free Guidance\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "In this tutorial, we'll explore how to customize a classifier-free guidance (CFG) model for a specific task. Let's first review how CFG works.\n",
    "\n",
    "### 1.1 Classifier-free Guidance\n",
    "We consider a conditional generation task, where we want to sample from a conditional distribution $q_0(\\bm x|\\bm y)$. The score function can be written as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla_{\\bm x}\\log q_t(\\bm x_t|\\bm y)=\\nabla_{\\bm x}\\log q_t(\\bm x_t) + \\nabla_{\\bm x}\\log q_t(\\bm y|\\bm x_t),\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the first term on the right side is the score function of unconditional distribution $q_t(\\bm x_t)$ that can be estimated by training an unconditional diffusion model on the dataset. The second term is what the guidance methods need to estimate. CFG uses $\\nabla_{\\bm x}\\log q_t(\\bm y|\\bm x_t)=\\nabla_{\\bm x}\\log q_t(\\bm x_t|\\bm y)-\\nabla_{\\bm x}\\log q_t(\\bm x_t)$. By training a conditional noise prediction model $\\bm\\epsilon_\\theta(\\bm x_t, t, \\bm y)$, the sampling process can be guided with no additional classifier:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\bar{\\bm\\epsilon_\\theta}(\\bm x_t, t, \\bm y)=\\bm\\epsilon_\\theta(\\bm x_t, t)-w\\cdot\\left(\\bm\\epsilon_\\theta(\\bm x_t, t, \\bm y)-\\bm\\epsilon_\\theta(\\bm x_t, t)\\right),\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $w$ is the guidance strength. In practice, we use a dummy condition $\\bm y=\\bm\\Phi$ to represent unconditional generation, i.e., $\\bm\\epsilon_\\theta(\\bm x_t, t,\\bm\\Phi)=\\bm\\epsilon_\\theta(\\bm x_t, t)$.\n",
    "\n",
    "In decision-making, the condition $\\bm y$ may be highly complex multi-modal data, e.g., image-based observations, language instructions, point clouds, and so on. Some works even use large transformers for multimodal fusion processing of conditions, while using small MLPs as the diffusion NN backbone. Therefore, in CleanDiffuser, we believe it is forward-looking and necessary to decouple the neural networks of Diffusion $\\bm\\epsilon_\\theta$ and the conditions $\\bm\\zeta_\\phi$ to facilitate development and debugging. The conditional diffusion models in CleanDiffuser are actually implemented as $\\epsilon_\\theta(\\bm x_t, t, \\bm\\zeta_\\phi(\\bm y))$, and the dummy condition is defined to be zeros $\\bm\\zeta_\\phi(\\bm\\Phi)=\\bm 0$ without loss of generality. This is why in tutorial 1, we need both a `NNDiffusion` and a `NNCondition` to create a diffusion model. The `NNDiffusion` is the $\\bm\\epsilon_\\theta$ here and the `NNCondition` is the $\\bm\\zeta_\\phi$.\n",
    "\n",
    "### 1.2 Diffusion Planners\n",
    "\n",
    "In this tutorial, we'll implement a diffusion planner using CFG. The basic idea of diffusion planners is to generate high-performance decision trajectories and extract the first action in the trajectory to execute. This is actually very similar to MPC and many planning-based model-based RL algorithms. They use searching methods and dynamic models to obtain high-performance trajectories, while diffusion planners use conditional generation to achieve this.\n",
    "\n",
    "Obviously, we need a \"high-performance\" variable as a condition to guide the generation. A simple and commonly used method is to use the discounted return-to-go of trajectories $\\sum_{s=t}^T \\gamma^{s-t} r_s$ in the dataset as the condition. It is actually a Monte Carlo estimation of the value of the trajectory. During training, we normalize the values in the dataset to the range [0, 1], so that a value of 1 represents the highest performance. During inference, we use relatively high normalized values like 0.8-1.0 as conditions to generate high-performance trajectories. For more details, we recommend reading [Diffuser](https://arxiv.org/abs/2205.09991) and [Decision Diffuser](https://openreview.net/forum?id=sP1fo2K9DFG).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Setting up the Environment and Preparing the Dataset\n",
    "\n",
    "We use D4RL-MuJoCo-halfcheetah-medium-expert-v2 as the benchmark. D4RL-MuJoCo is a widely used offline RL benchmark. `halfcheetah-medium-expert-v2` requires to control a halfcheetah robot to move forward as fast as possible, and it provides a medium-expert-quality demonstration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: All of online environments libraries in D4RL have been moved \u001b]8;;https://github.com/Farama-Foundation/Gymnasium\u001b\\Gymnasium\u001b]8;;\u001b\\, \u001b]8;;https://github.com/Farama-Foundation/MiniGrid\u001b\\MiniGrid\u001b]8;;\u001b\\ and \u001b]8;;https://github.com/Farama-Foundation/Gymnasium-Robotics\u001b\\Gymnasium-Robotics\u001b]8;;\u001b\\, and all offline datasets in D4RL have been moved to \u001b]8;;https://github.com/Farama-Foundation/Minari\u001b\\Minari\u001b]8;;\u001b\\.\n",
      "These new versions include large bug fixes, new versions of Python, and are where all new development will continue. Please upgrade these libraries as soon as you're able to do so.\n",
      "If you'd like to read more about the story behind this switch, please check out \u001b]8;;https://farama.org/Announcing-Minari\u001b\\this blog post\u001b]8;;\u001b\\.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "\nMissing path to your environment variable. \nCurrent values LD_LIBRARY_PATH=/home/users/zhiwei/miniconda3/envs/cleandiffuser/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64::/home/users/zhiwei/.mujoco/mujoco210/bin\nPlease add following line to .bashrc:\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:/home/users/zhiwei/.mujoco/mujoco210/bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01md4rl\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcleandiffuser\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01md4rl_mujoco_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m D4RLMuJoCoDataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# horizon=4 is enough for halfcheetah tasks as mentioned in Diffuser paper.\u001b[39;00m\n",
      "File \u001b[0;32m~/d4rl/D4RL/d4rl/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m _ERROR_MESSAGE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocomotion\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhand_manipulation_suite\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpointmaze\u001b[39;00m\n",
      "File \u001b[0;32m~/d4rl/D4RL/d4rl/locomotion/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocomotion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ant\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocomotion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m maze_env\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mregister(\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    id='antmaze-umaze-v0',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/d4rl/D4RL/d4rl/locomotion/ant.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmujoco_py\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/miniconda3/envs/cleandiffuser/lib/python3.9/site-packages/mujoco_py/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cymj, ignore_mujoco_warnings, functions, MujocoException\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m const\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmjrenderpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MjRenderPool\n",
      "File \u001b[0;32m~/miniconda3/envs/cleandiffuser/lib/python3.9/site-packages/mujoco_py/builder.py:504\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m__fun\n\u001b[1;32m    503\u001b[0m mujoco_path \u001b[38;5;241m=\u001b[39m discover_mujoco()\n\u001b[0;32m--> 504\u001b[0m cymj \u001b[38;5;241m=\u001b[39m \u001b[43mload_cython_ext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmujoco_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Trick to expose all mj* functions from mujoco in mujoco_py.*\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdict2\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/cleandiffuser/lib/python3.9/site-packages/mujoco_py/builder.py:76\u001b[0m, in \u001b[0;36mload_cython_ext\u001b[0;34m(mujoco_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m _ensure_set_env_var(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, lib_path)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMUJOCO_PY_FORCE_CPU\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_nvidia_lib_dir() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[43m_ensure_set_env_var\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLD_LIBRARY_PATH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_nvidia_lib_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     Builder \u001b[38;5;241m=\u001b[39m LinuxGPUExtensionBuilder\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cleandiffuser/lib/python3.9/site-packages/mujoco_py/builder.py:120\u001b[0m, in \u001b[0;36m_ensure_set_env_var\u001b[0;34m(var_name, lib_path)\u001b[0m\n\u001b[1;32m    118\u001b[0m paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib_path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMissing path to your environment variable. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent values \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add following line to .bashrc:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=$\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (var_name, os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(var_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    124\u001b[0m                                           var_name, var_name, lib_path))\n",
      "\u001b[0;31mException\u001b[0m: \nMissing path to your environment variable. \nCurrent values LD_LIBRARY_PATH=/home/users/zhiwei/miniconda3/envs/cleandiffuser/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64::/home/users/zhiwei/.mujoco/mujoco210/bin\nPlease add following line to .bashrc:\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = os.environ.get(\"LD_LIBRARY_PATH\", \"\") + \":/home/users/zhiwei/.mujoco/mujoco210/bin\"\n",
    "\n",
    "import gym\n",
    "import d4rl\n",
    "from cleandiffuser.dataset.d4rl_mujoco_dataset import D4RLMuJoCoDataset\n",
    "\n",
    "\n",
    "# horizon=4 is enough for halfcheetah tasks as mentioned in Diffuser paper.\n",
    "horizon = 4\n",
    "env = gym.make(\"halfcheetah-medium-expert-v2\")\n",
    "dataset = D4RLMuJoCoDataset(env.get_dataset(), terminal_penalty=-100, horizon=horizon)\n",
    "obs_dim, act_dim = dataset.o_dim, dataset.a_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Customizing a CFG Model\n",
    "\n",
    "To customize a CFG model, we are actually designing the condition network $\\bm\\zeta_\\phi$. So our first step is to check how the diffusion network uses the condition embedding. Suppose we use `DiT1d` as the diffusion network and we take a look at the forward function of `DiT1d` to see it takes a tensor of shape `(batch_size, embedding_dim)` as the condition embedding. And we use the value tensor of shape `(batch_size, 1)` as the generation condition. So we need to design a condition network that can map the value tensor to the condition embedding tensor. Here we use a simple MLP as the condition network (See `ValueNNCondition` below). Then we can simply combine the `DiT1d` and `ValueNNCondition` to create a diffusion model as we did in tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleandiffuser.nn_condition import BaseNNCondition, get_mask\n",
    "from cleandiffuser.utils import at_least_ndim\n",
    "from cleandiffuser.nn_diffusion import DiT1d\n",
    "from cleandiffuser.diffusion import ContinuousDiffusionSDE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ValueNNCondition(BaseNNCondition):\n",
    "    \"\"\" Simple MLP NNCondition for value conditioning.\n",
    "    \n",
    "    value (bs, 1) -> ValueNNCondition -> embedding (bs, emb_dim)\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int): Embedding dimension.\n",
    "        dropout (float): Label dropout rate.\n",
    "    \n",
    "    Example:\n",
    "        >>> value = torch.rand(32, 1)\n",
    "        >>> condition = ValueNNCondition(emb_dim=64, dropout=0.25)\n",
    "        >>> # If condition.training, embedding will be masked to be dummy condition \n",
    "        >>> # with label dropout rate 0.25.\n",
    "        >>> embedding = condition(value) \n",
    "        >>> embedding.shape\n",
    "        torch.Size([32, 64])\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim: int, dropout: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1, 256), nn.SiLU(),\n",
    "            nn.Linear(256, 256), nn.SiLU(),\n",
    "            nn.Linear(256, emb_dim))\n",
    "    def forward(self, condition: torch.Tensor, mask: torch.Tensor = None):\n",
    "        mask = get_mask(\n",
    "            mask, (condition.shape[0],), self.dropout, self.training, condition.device)\n",
    "        mask = at_least_ndim(mask, condition.dim())\n",
    "        return condition * mask\n",
    "    \n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "nn_diffusion = DiT1d(\n",
    "    obs_dim + act_dim, emb_dim=128, d_model=320, n_heads=10, depth=2, \n",
    "    timestep_emb_type=\"untrainable_fourier\")\n",
    "nn_condition = ValueNNCondition(emb_dim=128, dropout=0.25)\n",
    "\n",
    "fix_mask = torch.zeros((horizon, obs_dim + act_dim))\n",
    "fix_mask[0, :obs_dim] = 1.\n",
    "loss_weight = torch.ones((horizon, obs_dim + act_dim))\n",
    "loss_weight[0, obs_dim:] = 10.\n",
    "\n",
    "planner = ContinuousDiffusionSDE(\n",
    "    nn_diffusion=nn_diffusion, nn_condition=nn_condition,\n",
    "    fix_mask=fix_mask, loss_weight=loss_weight, ema_rate=0.9999,\n",
    "    device=device)\n",
    "\n",
    "random_obs = torch.randn((obs_dim,))\n",
    "prior = torch.zeros((1, horizon, obs_dim + act_dim))\n",
    "prior[:, 0, :obs_dim] = random_obs[None, :]\n",
    "\n",
    "traj, log = planner.sample(\n",
    "    prior, solver=\"ddpm\", n_samples=1, sample_steps=5)\n",
    "\n",
    "print(f'Trajectory shape: {traj.shape}')\n",
    "print(f'First observation MSE: {(traj[0, 0, :obs_dim].cpu() - random_obs).pow(2).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that we use some new variables like `fix_mask` and `loss_weight` that we didn't use in tutorial 1. Let's explain them here. \n",
    "\n",
    "The diffusion-generated trajectories are looks like:\n",
    "$$\n",
    "\\bm\\tau = \\left[\n",
    "\\begin{aligned}\n",
    "&\\bm s_0, \\bm s_1, \\cdots, \\bm s_{H-1} \\\\\n",
    "&\\bm a_0, \\bm a_1, \\cdots, \\bm a_{H-1} \\\\\n",
    "\\end{aligned}\n",
    "\\right],\n",
    "$$\n",
    "where $\\bm s_0$ is the current state and it is known and fixed during generation. So the generation process works like an image inpainting task. `fix_mask` is a tensor with the same shape as $\\bm\\tau$ and it is 1 for known items and 0 for unknown items. During training, the fixed parts are maintained and not contributed to the loss. During inference, the fixed parts in `prior` are used to do inpainting. This is why we set `prior[:, 0, :obs_dim] = obs` before sampling.\n",
    "\n",
    "`loss_weight` is also a tensor with the same shape as $\\bm\\tau$ and it is used to weight the loss. In this tutorial, since the first action $\\bm a_0$ directly affects the decision-making performance, we set the weight of the first action to be 10 times larger than the other parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training the Diffusion Model\n",
    "\n",
    "This part is almost the same as tutorial 1, except that the generated data is the trajectory and the generation condition is the value. You may find it strange that we divide the value tensor by 1200. The 1200 is actually an empirical value that makes the value tensor in the range [0, 1], and is observed in the dataset. It's may be a little bit dumb, but it is simple and works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from cleandiffuser.utils import loop_dataloader\n",
    "\n",
    "\n",
    "savepath = \"../tutorials/results/2_classifier_free_guidance/\"\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "\n",
    "n_gradient_steps = 0\n",
    "avg_loss = 0.\n",
    "planner.train()\n",
    "for batch in loop_dataloader(dataloader):\n",
    "    \n",
    "    obs, act = batch[\"obs\"][\"state\"].to(device), batch[\"act\"].to(device)\n",
    "    val = batch[\"val\"].to(device) / 1200.\n",
    "    x0 = torch.cat([obs, act], dim=-1)\n",
    "\n",
    "    avg_loss += planner.update(x0=x0, condition=val)[\"loss\"]\n",
    "    \n",
    "    n_gradient_steps += 1\n",
    "    \n",
    "    if n_gradient_steps % 1000 == 0:\n",
    "        print(f'Step: {n_gradient_steps} | Loss: {avg_loss / 1000}')\n",
    "        avg_loss = 0.\n",
    "    \n",
    "    if n_gradient_steps % 100_000 == 0:\n",
    "        planner.save(savepath + \"diffusion.pt\")\n",
    "    \n",
    "    if n_gradient_steps == 500_000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Evaluation\n",
    "\n",
    "Let's see how our customized CFG planner performs in `halfcheetah-medium-expert-v2`! We parallelly interact with 50 environments and use 3 random seeds to evaluate the performance. The evaluation metric is the normalized episode return, with 100 being the expert-performance and 0 being the random-performance. We use DDPM with 5 sampling steps (compared to 100 sampling steps used in Decision Diffuser official implementation) to generate trajectories. The results show that we can achieve a D4RL score of 88.4. For a model without carefully tuning, this is not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "solver = \"ddpm\"\n",
    "sampling_step = 5\n",
    "num_episodes = 3\n",
    "num_envs = 50\n",
    "target_return = 0.95\n",
    "w_cfg = 1.2\n",
    "\n",
    "planner.load(savepath + \"diffusion.pt\")\n",
    "planner.eval()\n",
    "\n",
    "# Parallelize evaluation\n",
    "env_eval = gym.vector.make('halfcheetah-medium-expert-v2', num_envs=num_envs)\n",
    "\n",
    "# Get normalizers\n",
    "normalizer = dataset.get_normalizer()\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "prior = torch.zeros((num_envs, horizon, obs_dim + act_dim), device=device)\n",
    "condition = torch.ones((num_envs, 1), device=device) * target_return\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    obs, ep_reward, cum_done, t = env_eval.reset(), 0., 0., 0\n",
    "\n",
    "    while not np.all(cum_done) and t < 1000 + 1:\n",
    "        \n",
    "        # normalize obs\n",
    "        obs = torch.tensor(normalizer.normalize(obs), device=device, dtype=torch.float32)\n",
    "\n",
    "        # sample trajectories\n",
    "        prior[:, 0, :obs_dim] = obs\n",
    "        traj, log = planner.sample(\n",
    "            prior, \n",
    "            solver=solver,\n",
    "            n_samples=num_envs, \n",
    "            sample_step_schedule=\"quad_continuous\",\n",
    "            sample_steps=sampling_step, use_ema=True,\n",
    "            condition_cfg=condition, w_cfg=w_cfg, temperature=1.0)\n",
    "        act = traj[:, 0, obs_dim:].clip(-1., 1.).cpu().numpy()\n",
    "\n",
    "        # step\n",
    "        obs, rew, done, info = env_eval.step(act)\n",
    "\n",
    "        t += 1\n",
    "        cum_done = done if cum_done is None else np.logical_or(cum_done, done)\n",
    "        ep_reward += (rew * (1 - cum_done)) if t < 1000 else rew\n",
    "        print(f'[t={t}] rew: {np.around((rew * (1 - cum_done)), 2)}')\n",
    "\n",
    "    episode_rewards.append(ep_reward)\n",
    "\n",
    "episode_rewards = [list(map(lambda x: env.get_normalized_score(x), r)) for r in episode_rewards]\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "mean_rewards = np.mean(episode_rewards, -1) * 100.\n",
    "print(f'D4RL score: {mean_rewards.mean():.3f} +- {mean_rewards.std():.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleandiffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
